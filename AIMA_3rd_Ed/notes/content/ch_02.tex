% !TEX root = ../
\chapter{Intelligent Agents} % (fold)
\label{cha:intelligent_agents}

Chapter \ref{cha:introduction} referred to rational agents, here we define them.

\section{Agents and Environments} % (fold)
\label{sec:agents_and_environments}

Agents perceive their environment through sensors and act on it with actuators.
A ``percept'' is the agent's perceptual inputs at any given instant.
``Percept sequences'' are time-series of percept states.
``Agent functions'' describe the percept sequence to action mapping of an agent.
Agent functions are implemented as programs.
Agents are meant to be a tool for analyzing systems.

% section agents_and_environments (end)

\section{Good Behaviour - The Concept of Rationality} % (fold)
\label{sec:good_behaviour_the_concept_of_rationality}

Rational agents do the right thing, but what is the right thing?
Consequences of behaviour is a big measure.
Sometimes there is a performance measure to evaluate the environment state.
Some measures can be implemented dumbly, they need to be carefully designed.

\subsection{Rationality} % (fold)
\label{sub:rationality}
Rationality depends on four tenets:
\begin{enumerate}
    \item The performance that defines the criterion of success.
    \item The agent's prior knowledge of environment.
    \item The actions performable by the agent.
    \item The agent's percept sequence to date.
\end{enumerate}

Rational agents can be defined as follows:

\begin{em}
    For each possible percept sequence, a rational agent should select an action
    that is expected to maximize its performance measure, given the evidence
    provided by the percept sequence and whatever built-in knowledge the agent
    has.
\end{em}

It is necessary and sufficient that agents that fulfill all four tenets to be
rational.

% subsection rationality (end)

\subsection{Omniscience, Learning, and Autonomy} % (fold)
\label{sub:omniscience_learning_and_autonomy}

Omniscient agents are not possible within reality, because they \uline{know} the
outcome of their actions.
Rational agents draw conclusions about everything they \uline{can} know, not
everything knowable.
Rational agents then do information gathering to be better at predicting
consequences.
Rational agents should learn what they should expect to have to compensate for.
Applying learned knowledge is a prerequisite to success in many environments.

% subsection omniscience_learning_and_autonomy (end)

% section good_behaviour_the_concept_of_rationality (end)

\section{The Nature of Environments} % (fold)
\label{sec:the_nature_of_environments}

We know about rationality, let's talk about environment.

\subsection{Specifying the Task Environment} % (fold)
\label{sub:specifying_the_task_environment}

Task Environments are tuples of performance, environment, actuators, and
sensors. (PEAS)
Some task environments like the Roomba are very simple to define while others
like taxicab drivers are very hard.

% subsection specifying_the_task_environment (end)

\subsection{Properties of Task Environments} % (fold)
\label{sub:properties_of_task_environments}

We can categorize task environments into a few broad categories:
\begin{itemize}
    \item \uline{Fully v.s. Partially observable} environments - if the sensors
    can see everything, then the task is fully observable.
    \item \uline{Single Agent v.s. Multiagent} - if the agent is alone, or if
    there are multiple agents playing against it.
    \item \uline{Deterministic v.s. Stochaistic} - if the next step is
    completely determined by the current state and action, or if there is some
    sort of randomness involved.
    \item \uline{Episodic v.s. Sequential} -
    Episodic task environments are one-off episodes with consequences of actions.
    Sequential task environments are where short-term actions have consequences.
    \item \uline{Static v.s. Dynamic} -
    Static environments do not change while computation occurs.
    Dynamic environments  are where waiting for computation is choosing to
    ``do nothing''.
    \item \uline{Discrete v.s. Continuous} -
    State, time, and percepts can either be continuously occurring, or
    distinctly different states, times, and percepts.
    \item \uline{Known v.s. Unknown} -
    In known environments, outcomes are known.
    In unknown environments, agents will need to understand how it works to make
    good decisions.
\end{itemize}

Expectedly, the hardest environment is Partially Observable, Multiagent,
Stochaistic, Sequential, Dynamic, Continuous and Unknown.
Just like driving a car in an unfamiliar country, with unknown driving laws.

% subsection properties_of_task_environments (end)

% section the_nature_of_environments (end)

\section{The Structure of Agents} % (fold)
\label{sec:the_structure_of_agents}

The job of AI is to design agent programs to map precepts to actions.
Agents are compromised of a physical sensor/actuator architecture and a program.

\subsection{Agent Programs} % (fold)
\label{sub:agent_programs}

All agents in this book take current precept as input and control actuators.
Agents use current percept as input, agent functions use the entire history.
Table-Driven agents implement the desired agent function, but are impossibly
hard to populate tables for.
Tables of sqrts have been replaced by a five-line newtonian regression method on
electronic calculators.
The question is: can AI do for general intelligent behavior what Newton did for
square roots? We think yes.

% subsection agent_programs (end)

\subsection{Simple Reflex Agents} % (fold)
\label{sub:simple_reflex_agents}

Simple reflex agents select actions on the basis of current precept only.
These are always very simple (e.g. if car-ahead-is-braking then start-braking).
They are simple, but they have very limited intelligence; partial observability
kills these agents.

% subsection simple_reflex_agents (end)

\subsection{Model-Based Reflex Agents} % (fold)
\label{sub:model_based_reflex_agents}

The most effective way to handle partial observability is to keep an internal
model of what the agent can't see.
These agents need to understand the wold evolving around them, and how their
actions affect the world.

% subsection model_based_reflex_agents (end)

\subsection{Goal-Based Agents} % (fold)
\label{sub:goal_based_agents}

Agents need a goal to base decisions towards.
They need to understand some situations as desirable, and others as less.
Goal-based action is sometimes straightforward, but is often not.
This involves decision about the future (e.g. What will make me happy?).
Goal-based agents appear less efficient, but knowledge makes them more flexible.

% subsection goal_based_agents (end)

\subsection{Utility-Based Agents} % (fold)
\label{sub:utility_based_agents}

Goals are not enough to generate high-quality behavior, as they are only binary.
Utility is a measure of ``how good'' a state is, computed by a utility function.
This is not the only way to be rational, but it is much more flexible.
\begin{em}
    Rational utility-based agents choose the action that maximize the expected
    utility the agent expects to derive, on average, given the probabilities and
    utilities of each outcome.
\end{em}
Utility functions allow tradeoffs between different (possibly uncertain) goals.

% subsection utility_based_agents (end)

\subsection{Learning Agents} % (fold)
\label{sub:learning_agents}

Turing proposes that we can build learning machines by building then teaching.
\uline{Learning elements} are responsible for making improvements;
\uline{Performance elements} are responsible for selecting external actions.
A \uline{Critic} compares of the learning element to an external standard.
\uline{Problem Generator}s disrupt the monotony of learned things with actions
that lead to informative, new experiences.

% subsection learning_agents (end)

\subsection{How the Components of Agent Programs Work} % (fold)
\label{sub:how_the_components_of_agent_programs_work}

There are a few ways to represent the states of agent programs:
\begin{itemize}
    \item Atomic Representation - each state has no internal structure.
    \item Factored Representation - splits up states into a set of attributes.
    \item Structured Representation - world is things that are related, not just
    variables with value.
\end{itemize}
The more expressive a representation, the more complex learning and reasoning
become.

% subsection how_the_components_of_agent_programs_work (end)

\subsection{Summary} % (fold)
\label{sub:summary}

This chapter tours AI really quickly, but here are some key points:
\begin{itemize}
    \item Agents perceive and act in an environment.
    \item An agent function for an agent specifies the action taken in response
    to any percept sequence.
    \item The performance measure evaluates behaviour of the agent.
    \item A rational agent acts to maximize the expected value of an action
    given the precept sequence so far.
    \item Task environments include the performance measure, external
    environment, the actuators, and the sensors.
    \item To design agents, we should specify task environments first.
    \item Task environments vary along dimensions. (agents, determinsitic, etc.)
    \item The agent program implements the agent function.
    \item There are a bunch of basic agent-program designs reflecting info used
    in the decision process.
    \item Simple reflex agents respond to percepts, model-based reflex agents
    maintain a model.
    Goal-based agents act to achieve goals, utility-based
    agents act to maximize utility (``happiness''.)
    \item All agents can improve through learning.
\end{itemize}

% subsection summary (end)

% section the_structure_of_agents (end)

% chapter intelligent_agents (end)